<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Noor Uddin - Publications</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
</head>
<body>

    <header>
        <div class="header-container">
            <h1>Nooruddin Noonari</h1>
            <nav>
                <ul>
                    <li><a href="index.html">About</a></li>
                    <li><a href="experience.html">Experience</a></li>
                    <li><a href="projects.html">Projects</a></li>
                    <li><a href="publications.html">Publications</a></li>
                    <li><a href="certificates.html">Certificates</a></li>
                    <li><a href="awards.html">Awards</a></li>
                    <li><a href="summer_schools.html">Summer Schools</a></li>
                    <li><a href="news.html">News</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <div class="container">
        <aside class="sidebar">
            <img src="imgs/pic.png" alt="Noor Uddin" class="profile-img">
            <h2>Nooruddin Noonari</h2>
            <p class="no-justify">Investigator (DL/ML) @ DME</p>
            <p class="no-justify">PhD Candidate under MSCA</p>
            <p><i class="fas fa-map-marker-alt icon"></i> Aveiro, Portugal</p>
            <a href=""><i class="fas fa-envelope icon"></i> <strong>Email: nooruddin@ua.pt</strong></a>
            <a href="https://x.com/noorcs39"><i class="fab fa-twitter icon" target="_blank"></i> Twitter</a>
            <a href="https://www.linkedin.com/in/noonari/"><i class="fab fa-linkedin icon" target="_blank"></i> LinkedIn</a>
            <a href="https://github.com/noorcs39"><i class="fab fa-github icon" target="_blank"></i> Github</a>
            <a href="https://scholar.google.com/citations?user=h5RrjHwAAAAJ&hl=en"><i class="fas fa-graduation-cap icon" target="_blank"></i> Google Scholar</a>
            <a href="https://orcid.org/0000-0002-3843-0415"><i class="fas fa-id-card icon" target="_blank"></i> ORCID</a>
        </aside>
        <div class="content">
            <section id="publications">
                <h1>Publications</h1>
                
                <h2>2024:</h2>
                <div class="publication">
                    <h3><a href="https://ieeexplore.ieee.org/abstract/document/10543537/" class="publication-title">Color Recognition in Challenging Lighting Environments: CNN Approach</a></h3>
                    <ul>
                        <li><p class="authors">Nizamuddin Maitlo; <strong>Nooruddin Noonari</strong>; Sajid Ahmed Ghanghro; Sathishkumar Duraisamy; Fayaz Ahmed</p></li>
                        <li><p><em>arXiv</em> <a href="https://arxiv.org/abs/2402.04762" class="highlight-link">[paper]</a></p></li>
                        <li><p class="description">Our research introduces a novel approach to enhance the robustness of color detection in varying lighting conditions by leveraging a Convolutional Neural Network (CNN). Specifically, we maintain the integrity of image segmentation using edge detection techniques to isolate objects and then incorporate a CNN trained to detect object colors under diverse lighting conditions. This approach not only preserves the accuracy of object segmentation but also optimizes the color detection capabilities within the network, leading to significantly improved performance compared to existing methods.</p></li>
                    </ul>
                </br>
                    <h3><a href="https://ieeexplore.ieee.org/abstract/document/10544260" class="publication-title">AINS: Affordable Indoor Navigation Solution via Line Color Identification Using Mono-Camera for Autonomous Vehicles</a></h3>
                    <ul>
                        <li><p class="authors">Nizamuddin Maitlo; <strong>Nooruddin Noonari</strong>; Kaleem Arshid; Naveed Ahmed; Sathishkumar Duraisamy</p></li>
                        <li><p><em>arXiv</em> <a href="https://arxiv.org/abs/2402.04750" class="highlight-link">[paper]</a></p></li>
                        <li><p class="description">Our research introduces a novel approach to enhance the effectiveness and efficiency of indoor navigation for autonomous vehicles by leveraging a low-cost solution. Specifically, we retain the fundamental navigation capabilities of the base system and incorporate a monocular camera to address the challenges posed by GPS inaccuracy in indoor scenarios. Our solution, termed Affordable Indoor Navigation Solution (AINS), is designed to function without the need for extensive or power-inefficient sensors such as range finders, thereby maintaining cost-effectiveness. Through this approach, our method demonstrates superior performance over existing solutions, significantly reducing both estimation errors and time consumption.</p></li>
                    </ul>
                </div>
            </br>
                <h2>2021:</h2>
                <div class="publication">
                    <h3><a href="" class="publication-title">Comparative Analysis of Software Process Models in Software Development</a></h3>
                    <ul>
                        <li><p class="authors"> Sajid Ahmed Ghanghro, Dr. Muhammad Ajmal Sawand Wajid Ahmed Channa, Ubaidulla alias Kashif, Muhammad Hanif Tunio, Kishor Kumar, <strong>Nooruddin Noonari</strong></p></li>
                    </ul>
                </br>
                    <h2>2020:</h2>
                    <div class="publication">
                        <h3><a href="https://ieeexplore.ieee.org/abstract/document/9283348" class="publication-title">HGR: Hand-gesture-recognition based text input method for AR/VR wearable devices</a></h3>
                        <ul>
                            <li><p class="authors"><strong>Nooruddin Noonari</strong>; Rahool Dembani; Nizamuddin Maitlo; </p></li>
                            <li><p><em>arXiv</em> <a href="https://dl.acm.org/doi/abs/10.1109/SMC42975.2020.9283348" class="highlight-link">[paper]</a></p></li>
                            <li><p class="description">Our research introduces a novel approach to enhance the effectiveness and efficiency of indoor navigation for autonomous vehicles by leveraging a low-cost solution. Specifically, we retain the fundamental navigation capabilities of the base system and incorporate a monocular camera to address the challenges posed by GPS inaccuracy in indoor scenarios. Our solution, termed Affordable Indoor Navigation Solution (AINS), is designed to function without the need for extensive or power-inefficient sensors such as range finders, thereby maintaining cost-effectiveness. Through this approach, our method demonstrates superior performance over existing solutions, significantly reducing both estimation errors and time consumption.</p></li>
                        </ul>
                    </br>
                        <h3><a href="http://nnw.cz/doi/2020/NNW.2020.30.005.pdf" class="publication-title">UNSUPERVISED FACIAL EXPRESSION DETECTION USING GENETIC ALGORITHM</a></h3>
                        <ul>
                            <li><p class="authors">R Dembani, W Zheng <strong>Nooruddin Noonari</strong></p></li>
                            <li><p><em>arXiv</em> <a href="http://nnw.cz/doi/2020/NNW.2020.30.005.pdf" class="highlight-link">[paper]</a></p></li>
                            <li><p class="description">Our study proposes a novel approach to enhance the accuracy and efficiency of facial expression clustering by employing a genetic algorithm. Specifically, we convert images into a binary format to enable the effective selection of related clusters through various phases of the genetic algorithm. The proposed method integrates a modified teacher learning-based optimization algorithm, which updates the population in each phase to identify the most representative features. We utilize a real dataset of facial expressions to validate our approach and compare its performance against existing models across different evaluation parameters. Our findings demonstrate that the proposed method significantly improves precision, recall, and accuracy in facial expression identification without requiring any training.</p></li>
                        </ul>
                    </div>        
                <!-- Add more publications below, grouped by year as needed -->
            </section>
        </div>
    </div>

</body>
</html>
